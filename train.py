# -*- coding: utf-8 -*-
"""train.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1MAHVkjrfeZqOK0So7D5UxcF4zMq2_nRP
"""

from tensorflow import keras as keras
from tensorflow.keras.utils import to_categorical
import numpy as np
import os, sys
from data_helper import read_data, sents2sequences
from model import summary_model
#from model_helper import plot_attention_weights
from logger import get_logger
from tensorflow.python.lib.io import file_io

project_path = os.getcwd()
if project_path not in sys.path:
    sys.path.append(project_path)

base_dir = os.getcwd()
logger = get_logger("model.train",os.path.join(base_dir, 'logs'))

batch_size = 64
hidden_size = 96
ip_timesteps, op_timesteps = 20, 20

def get_data(train_size, random_seed=100):
#randomly shuffle train/test

    ip_text = read_data('/tmp/data/text.txt')
    op_text = read_data('/tmp/data/summary.txt')
    logger.info('Length of text: {}'.format(len(ip_text)))

    op_text = ['sos ' + sent[:-1] + 'eos .'  if sent.endswith('.') else 'sos ' + sent + ' eos .' for sent in op_text]

    np.random.seed(random_seed)
    inds = np.arange(len(ip_text))
    np.random.shuffle(inds)

    train_inds = inds[:train_size]
    test_inds = inds[train_size:]
    tr_ip_text = [ip_text[ti] for ti in train_inds]
    tr_op_text = [op_text[ti] for ti in train_inds]

    ts_ip_text = [ip_text[ti] for ti in test_inds]
    ts_op_text = [op_text[ti] for ti in test_inds]

    return tr_ip_text, tr_op_text, ts_ip_text, ts_op_text

def preprocess_data(ip_tokenizer, op_tokenizer, ip_text, op_text, in_timesteps, op_timesteps):
#Preprocessing and generating sequence of word indices

    ip_seq = sents2sequences(ip_tokenizer, ip_text, reverse=False, padding_type='pre', pad_length=ip_timesteps)
    op_seq = sents2sequences(op_tokenizer, op_text, pad_length=op_timesteps)
    logger.info('Vocabulary size (Input): {}'.format(np.max(ip_seq)+1))
    logger.info('Vocabulary size (Output): {}'.format(np.max(op_seq)+1))
    logger.debug('IP text shape: {}'.format(ip_seq.shape))
    logger.debug('OP text shape: {}'.format(op_seq.shape))

    return ip_seq, op_seq

def train(full_model, ip_seq, op_seq, batch_size, n_epochs=10):
#Training the model

    for ep in range(n_epochs):
        losses = []
        for bi in range(0, ip_seq.shape[0] - batch_size, batch_size):

            ip_onehot_seq = to_categorical(ip_seq[bi:bi + batch_size, :], num_classes=ip_vsize)
            op_onehot_seq = to_categorical(op_seq[bi:bi + batch_size, :], num_classes=op_vsize)

            full_model.train_on_batch([ip_onehot_seq, op_onehot_seq[:, :-1, :]], op_onehot_seq[:, 1:, :])

            l = full_model.evaluate([ip_onehot_seq, op_onehot_seq[:, :-1, :]], op_onehot_seq[:, 1:, :],
                                    batch_size=batch_size, verbose=0)

            losses.append(l)
        if (ep + 1) % 1 == 0:
            logger.info("Loss in epoch {}: {}".format(ep + 1, np.mean(losses)))

if __name__ == '__main__':

    debug = True

    train_size = 100000 if not debug else 8000
    filename = ''

    tr_ip_text, tr_op_text, ts_ip_text, ts_op_text = get_data(train_size=train_size)

#Defining tokenizers
    ip_tokenizer = keras.preprocessing.text.Tokenizer(oov_token='UNK')
    ip_tokenizer.fit_on_texts(tr_ip_text)

    op_tokenizer = keras.preprocessing.text.Tokenizer(oov_token='UNK')
    op_tokenizer.fit_on_texts(tr_op_text)
    print("tokenized")
#Getting preprocessed data
    ip_seq, op_seq = preprocess_data(ip_tokenizer, op_tokenizer, tr_ip_text, tr_op_text, ip_timesteps, op_timesteps)

    ip_vsize = max(ip_tokenizer.index_word.keys()) + 1
    op_vsize = max(op_tokenizer.index_word.keys()) + 1
    print("data processed")
#Defining the full model
    full_model, infer_enc_model, infer_dec_model = summary_model(hidden_size=hidden_size, batch_size=batch_size,
        ip_timesteps=ip_timesteps, op_timesteps=op_timesteps,ip_vsize=ip_vsize, op_vsize=op_vsize)

    n_epochs = 1 if not debug else 1
    train(full_model, ip_seq, op_seq, batch_size, n_epochs)
    print("trained")
#save model
    full_model.save('summarizer.h5')
    #full_model.save('/tmp/', save_format='tf')
    print("model built")

    #Copy summarizer.h5 over to Google Cloud Storage
    with file_io.FileIO('summarizer.h5', mode='r+') as input_f:
        with file_io.FileIO('gs://attention-255608-newlondon-bucket/summarizer.h5', mode='w+') as output_f:
            output_f.write(input_f.read())
            print("Saved summarizer.h5 to GCS")


   